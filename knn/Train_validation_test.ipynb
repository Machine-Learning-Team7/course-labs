{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation, and Test Sets\n",
    "\n",
    "**Training:**\n",
    "Data that is used to directly train the model. \n",
    "\n",
    "**Validation:**\n",
    "Data that is used to select between different models; or different values of hyperparameters for the same model.\n",
    "\n",
    "**Test:**\n",
    "Data that is used only at the very end, once a final model has been chosen. This will give us the best estimate of how well our model will perform on new, unseen data.\n",
    "\n",
    "In this notebook you will gain some experience with splitting a data set into *training* and *validation* sets. We will assume that a *test* data set has already been removed. Some of the things you will explore: \n",
    "\n",
    "  * splitting data without shuffling\n",
    "  * splitting data with shuffling\n",
    "  * how to use the `random_state` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some feature data\n",
    "\n",
    "Here we will create a simple feature array with 10 samples and 2 features. The rows represent *samples* and the columns represent the *features*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(np.arange(20)).reshape(10, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some target data\n",
    "\n",
    "We now need to create some target data to correspond to our feature data. Remember that the first element of our target array corresponds to the outcome/target/answer for the first sample, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 1, 0, 0, 0, 1, 0, 1, 1, 0])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put data into dataframe format\n",
    "\n",
    "We can use Pandas to put the data into the dataframe format. On occasion, this may be the more natural way to store the data. For now, it will allow you an easy way to see what's happening when we do or don't shuffle the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"feature_1\":X[:, 0], \"feature_2\":X[:, 1], \"target\":y})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data without shuffling\n",
    "\n",
    "We will take advantage of many of the *convenience* functions that come with the `sklearn` package. Because splitting data into training and test sets is an essential part of any supervised learning problem, `sklearn` contains a function that allows us to do this easily: `train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function from the sklearn package so that we can use it\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# the 'train_test_split' function returns 4 arrays \n",
    "# feature data arrays for training and testing\n",
    "# target data arrays for training and testing\n",
    "\n",
    "# NOTE: the order is important\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at the split data\n",
    "\n",
    "The default for `train_test_split` is to use 75% of the data for training and 25% for testing. The exact percentage depends on the size of your dataset. For example, if you have 3 training samples, a 75/25 split is not possible, so you will get a 66.6/33.3 split. \n",
    "\n",
    "Take a look at the `X_train` array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The training data is a 2 dimensional array: \\n {X_train}\\n\")\n",
    "\n",
    "# .shape gives us the number of rows and columns\n",
    "print(f\"The shape of the training data array is {X_train.shape}.\")  \n",
    "print(f\"The shape of the complete data array is {X.shape}.\\n\")  \n",
    "\n",
    "# calculate percent of data included in training data\n",
    "pct = (X_train.shape[0] / X.shape[0]) * 100  # .shape[0] selects the number of rows/samples\n",
    "print(f\"The percent of data contained in the training data array is {pct}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the data was split in order; that is, the training set is made up of the first elements of the original data. Use `df` above to verify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing supervised learning, we need the correct target values for our training data. Take a look at `y_train` to verify that it contains the targets corresponding to the samples in `X_train`. Use `df` above to verify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train # this corresponds to the first 7 rows of the dataframe 'df' we created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data with shuffling\n",
    "\n",
    "The default setting for `train_test_split` is to first *shuffle* the data and then do the split. We will come back to why this is important; for now let's focus on the basics. *Shuffling* the data means that the *rows* (which represent the samples) are randomy re-ordered. \n",
    "\n",
    "To see how this works, first reprint our original feature array using the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Remember that shuffle=True is the default so we don't normally need to specify this\n",
    "# we include it here for clarity\n",
    "X_train_new, X_valid_new, y_train_new, y_valid_new = train_test_split(X, y, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at the split data after shuffling\n",
    "\n",
    "Take a look at the `X_train_new` array: \n",
    "-  What percent of the original data is in your training set?\n",
    "    - *Answer*: still 70%\n",
    "-  Note the order of the split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `y_train_new` contains the correct targets for your shuffled training data. Use `df` above to verify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new # in the original dataframe 'df', the row [0, 1] should have target = 1, the row [10, 11] should have target 1, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling is random \n",
    "\n",
    "Rerun the last three code cells and note how each time you get a slightly different training set. This is because `train_test_split` **randomly** re-orders the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlled randomness\n",
    "\n",
    "While we want the shuffling procedure to randomly re-order our data, this can cause unnecessary problems when we are trying to debug our code or for reproducibility; every time you run your code the training set will change, which means your machine learning model will change, etc. To avoid this problem, we can set the `random_state` parameter. This parameter allows for a random re-ordering, but the same re-ordering will occur everytime you run your code. \n",
    "\n",
    "Let's see this in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Remember that shuffle=True is the default so we don't normally need to specify this\n",
    "# we include it here for increased clarity\n",
    "X_train_rs, X_valid_rs, y_train_rs, y_valid_rs = train_test_split(X, y, shuffle = True, random_state = 12)\n",
    "\n",
    "print(X_train_rs)\n",
    "print(y_train_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code a few times to convince yourself that we get the same re-ordering every time we run the code. \n",
    "\n",
    "**NOTE**: It doesn't matter which number you use with the `random_state` parameter. Try changing the `12` above to some other integer and rerun the code a few times. \n",
    "\n",
    "  - Did you get the same ordering as you did with `random_state = 12`?\n",
    "      - *Answer*: You should see a different order\n",
    "  - Did you get the same ordering every time you ran the code with a new number?\n",
    "      - *Answer*: You should see the same order if you keep 'random_state' fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does this matter?\n",
    "\n",
    "To get a feel for why we would want to shuffle our training data, we will build a kNN classification model for the Iris dataset. For now, all we need to know is that this dataset has 4 features, with the target being which species of Iris each sample is. The task is to build a model that uses the 4 features to predict the species. (You'll get more details about this dataset in a later notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "X_iris = iris_data.data\n",
    "y_iris = iris_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_iris_train, X_iris_valid, y_iris_train, y_iris_valid = train_test_split(X_iris, y_iris, shuffle = False, train_size = 2/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build kNN model with `k=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "iris_clf = KNeighborsClassifier(n_neighbors=3)   \n",
    "iris_clf.fit(X_iris_train, y_iris_train)   \n",
    "\n",
    "iris_acc_no_shuffle = iris_clf.score(X_iris_valid, y_iris_valid)\n",
    "\n",
    "print(\"Iris validation set accuracy without shuffling: {:.2f}\".format(iris_acc_no_shuffle))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data WITH shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_iris_shuffle_train, X_iris_shuffle_valid, y_iris_shuffle_train, y_iris_shuffle_valid = train_test_split(X_iris, y_iris, shuffle = True, train_size = 2/3, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build kNN model with `k=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "iris_shuffle_clf = KNeighborsClassifier(n_neighbors=3)   \n",
    "iris_shuffle_clf.fit(X_iris_shuffle_train, y_iris_shuffle_train)   \n",
    "\n",
    "iris_acc_shuffle = iris_clf.score(X_iris_shuffle_valid, y_iris_shuffle_valid)\n",
    "\n",
    "print(\"Iris valid set accuracy without shuffling: {:.2f}\".format(iris_acc_shuffle))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "See if you can figure out why there would be such a difference in accuracy when all we did was shuffle the data before splitting it into train and test sets. (The function included below may help you see what's going on easier, although you can just look at the NumPy arrays to figure it out.) \n",
    "\n",
    "Hint: \n",
    " - When we do not shuffle the data, what does the data look like that is being used to train the kNN classifier? \n",
    "     - *Answer*: In the 3 code cells below, note how the complete data has 3 classes (0, 1, 2) but our training data only contained 2 classes (0, 1). This is because the complete data comes with the classes in order: the first 50 rows correspond to class 0, the second 50 correspond to class 1, and the last 50 correspond to class 2. So, we have trained a model by only showing it 2 of the classes. That means our model only thinks there are 2 classes and has learned to try and predict if a new data point is either class 0 or class 1. Then we have asked our model to make predictions on our test data, but that only includes samples of class 2, which our model knows nothing about because we never taught it about class 2! It should get an accuracy of 0 because it can only predict class 0 or 1 and those predictions for our test data will always be incorrect because all the test data corresponds to class 2. (See the target arrays below to see the classes contained in the complete, train, and test data.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target array for the complete data\n",
    "y_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target array for the UNSHUFFLED data\n",
    "y_iris_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test array for the UNSHUFFLED data\n",
    "y_iris_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - How is this different to the shuffled training data?\n",
    "     - *Answer*: In the 3 code cells below, note how the complete, train, and test data sets all contain the 3 classes (0, 1, 2). This means that when we trained the model, we showed it examples of all 3 classes so when we asked it to make predictions on the test data it was able to do much better than before because it was now able to predict for class 0, 1, or 2. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target array for the complete data\n",
    "y_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target array for the SHUFFLED data\n",
    "y_iris_shuffle_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation array for the SHUFFLED data\n",
    "y_iris_shuffle_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should there be any connection between the *train*, *test*, and corresponding *real world* data?\n",
    "     - *Answer*: Ideally we want the train and test data to be as representative of the real world data so that our model can generalize. In the UNSHUFFLED example above, we saw what can happen when the train data and the test data aren't good representations: our model does not generalize well and performed poorly on the test set. Beyond this, we also don't want any sort of bias in our train or test data. For example, if you want to build a model that predicts house prices for Windsor and our train and test data only contain houses that cost more that a million dollars, then our model will not perform well when asked to make a prediction for a typical house in Windsor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating all 3 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X_iris, y_iris, shuffle = True, test_size = 0.15, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23 / 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% validation (of original 150 samples) = 0.2 * 150 = 30 \n",
    "# SO, that means approximately 25% of X_train_valid\n",
    "(0.2 * 150) / 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, shuffle = True, test_size = 0.25, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
