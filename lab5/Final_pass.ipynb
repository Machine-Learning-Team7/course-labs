{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urG_6owfqwuS"
   },
   "source": [
    "## Final Pass\n",
    "\n",
    "\n",
    "**Notice: This notebook is a modification of [eng.ipynb](https://mlbook.explained.ai/notebooks/index.html) by Terence Parr and Jeremy Howard, which was used by permission of the author.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tcl3KFEXqwuT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from rfpimp_MC import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x13zTYO3qwuU"
   },
   "outputs": [],
   "source": [
    "def evaluate(X, y, n_estimators=50):\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    oob = rf.oob_score_\n",
    "    n = rfnnodes(rf)\n",
    "    h = np.median(rfmaxdepths(rf))\n",
    "    print(f\"OOB R^2 is {oob:.5f} using {n:,d} tree nodes with {h} median tree depth\")\n",
    "    return rf, oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj3GJ3jqqwuU"
   },
   "outputs": [],
   "source": [
    "def showimp(rf, X, y):\n",
    "    features = list(X.columns)\n",
    "    I = importances(rf, X, y, features=features)\n",
    "    plot_importances(I, color='#4575b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "Recall that in the last notebook we did the following:\n",
    "- converted all 'string' features to ordered categorical features;\n",
    "- label encoded all these features using the value of 0 to represent missing data;\n",
    "- fixed some remaining problems with `YearMade` and `MachineHoursCurrentMeter`; and \n",
    "- replaced missing numeric data by:\n",
    "    - adding a new boolean (`True` or `False`) feature to say whether or not that value was missing; \n",
    "    - replace missing values in the original feature with the median of all values for that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In our final work on this data we will:\n",
    "- Break up the `saledate` feature into its components: year, month, day, etc.;\n",
    "- Ordinal encode `ProductSize`;\n",
    "- One hot encode `Hydraulics_Flow` and `Enclosure`; and, \n",
    "- Take the logarithm of our target variable, `SalePrice`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkxZmXgdqwuU"
   },
   "source": [
    "### Create a New, More Stable Baseline\n",
    "\n",
    "To get a more stable baseline, which will be better to compare improvements in our model, we will use 150 decision tress in the random forest instead of the 50 used previously. With more trees, the variance in the OOB $R^2$ is reduced. That means, if we run the model many times the OOB $R^2$ won't change as much (due to randomness of random forest construction). This will give us more confidence that any improvement we see is due to the changes we have made to the data as opposed to just random variations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "l951ilWnqwuU",
    "outputId": "a5319b1e-95de-40d4-e71f-f003fb3cb64d"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_feather(\"bulldozer-train.feather\")\n",
    "df_raw = df_raw.iloc[-100000:]\n",
    "df = pd.read_feather(\"bulldozer-train-clean.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = df.drop(['SalePrice','saledate'], axis=1) \n",
    "y = df['SalePrice']\n",
    "\n",
    "rf, oob_all = evaluate(X, y, n_estimators=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the OOB score for our new baseline is slightly higher than what we achieved in the last notebook. This is due to the increased number of trees in the random forest model, since the accuracy tends to increase (up to a point) when more trees are used. \n",
    "\n",
    "Let's also recall the feature importances for this model as our next steps are based on exploring what more we can do with the most important features to see if we can improve our OOB score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showimp(rf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features from `saledate`\n",
    "\n",
    "We will now deal with the only `datetime` data type we have in our data: `saledate`. For this procedure we will use Pandas's built-in functionality for datetimes. After extracting the date components, we will convert the original `saledate` feature to an integer, which for most operating systems means the number of seconds since 1 January 1970. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split_dates(df,colname):\n",
    "    df[\"saleyear\"] = df[colname].dt.year\n",
    "    df[\"salemonth\"] = df[colname].dt.month\n",
    "    df[\"saleday\"] = df[colname].dt.day\n",
    "    df[\"saledayofweek\"] = df[colname].dt.dayofweek\n",
    "    df[\"saledayofyear\"] = df[colname].dt.dayofyear\n",
    "    df[colname] = df[colname].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_dates(df, 'saledate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(n=5)\n",
    "df_small[['saledate', 'saleyear', 'salemonth', 'saleday', 'saledayofweek', 'saledayofyear']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know beforehand what will be predictive so you should default to adding whatever you can, including things like holidays, or major events, etc. \n",
    "\n",
    "And now that we have `saleyear` we can calculate the age of each bulldozer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['saleyear'] - df['YearMade']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model and see if these new features have had any impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "rf, oob_dates = evaluate(X, y, n_estimators=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a decent increase in our OOB score. Let's also take a look at the importances, but first, since all the new features we have added are correlated, we will bundle them together as one meta-feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.drop('SalePrice',axis=1).columns)\n",
    "datefeatures = list(df.filter(regex=(\"sale*\")).columns)\n",
    "\n",
    "for f in datefeatures:\n",
    "    features.remove(f)\n",
    "    \n",
    "features.remove('YearMade')\n",
    "features.remove('age')\n",
    "features += [['YearMade','age']+datefeatures]\n",
    "\n",
    "I = importances(rf, X, y, features=features)\n",
    "plot_importances(I.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the Default Approach\n",
    "\n",
    "We have said that in practice, or not sure what else to do, we encode categorical features using label encoding, which just assigns an integer to each unique category without considering any order. We want to modify this if we know that the feature is important. In this case, we would use: \n",
    "- one hot encoding if the number of unique categories is small, say 10 or less; \n",
    "- use ordinal encoding if the feature is ordinal, that is, has a natural ordering. \n",
    "\n",
    "We will see examples of both below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encode `ProductSize`\n",
    "\n",
    "Now let's explore the `ProductSize` feature as it's the second most important according to our model. To see it's connection to our target we will look at the original encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['ProductSize'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_raw.fillna('nan') \n",
    "temp = temp.groupby('ProductSize')['SalePrice'].mean()\n",
    "temp.head()\n",
    "temp.sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The barplot shows that there is a relationship between `ProductSize` and `SalePrice`. This tells us that we should use an ordinal encoding for this feature instead of the label encoding we did before (just assigns an integer to each category value but doesn't respect the ordering). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {None:0, 'Mini':1, 'Compact':1, 'Small':2, 'Medium':3, 'Large / Medium':4, 'Large':5}\n",
    "\n",
    "df['ProductSize'] = df_raw['ProductSize'].map(sizes).values\n",
    "df['ProductSize'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "rf, oob_psize = evaluate(X, y, n_estimators=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us another increase, although small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "We will skip over the next most important feature, `fiProductClassDesc` (see [Section 8.5](https://mlbook.explained.ai/bulldozer-feateng.html#sec:8.5) for details on that feature), and consider `Enclosure` and `Hydraulics_Flow`.   \n",
    "\n",
    "We will use one-hot encoding on these features. This technique was outlined in the notebook **Part 1 - Categorical Variables**, so review the appropriate section there if you forget. \n",
    "\n",
    "Again, we will need to go back to the original data do this, as the cleaned data has already label encoded these features. Let's first verify that they are not ordinal and don't contain too many unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['Enclosure'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['Hydraulics_Flow'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like both features fit the criteria for one-hot encoding (although an argument could be made that `Hydraulics_Flow` may be ordinal, given the uncertainty we will assume it is not). Notice also that since we are using the original data we will need to normalize the representations of missing values before we do our encoding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Hydraulics_Flow`\n",
    "\n",
    "To handle this feature we will:\n",
    "- replace our label encoded version of this feature with the original values;\n",
    "- normalize the representation of mising values to `np.nan`; and, \n",
    "- create a dataframe of the one-hot encoded feature;\n",
    "- delete the original feature; \n",
    "- combine our cleaned data with the one-hot encoded dataframe. \n",
    "\n",
    "Note that missing values will be indicated by all `False` (or 0) values when one-hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hydraulics_Flow'] = df_raw['Hydraulics_Flow'].values\n",
    "df['Hydraulics_Flow'] = df['Hydraulics_Flow'].replace('None or Unspecified', np.nan)\n",
    "\n",
    "onehot = pd.get_dummies(df['Hydraulics_Flow'], prefix='Hydraulics_Flow', dtype=bool)\n",
    "onehot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Hydraulics_Flow']\n",
    "\n",
    "df = pd.concat([df, onehot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Enclosure`\n",
    "\n",
    "To handle this feature we will:\n",
    "- replace our label encoded version of this feature with the original values;\n",
    "- clean up the data as two of the category values represent the same category;\n",
    "- normalize the representation of mising values to `np.nan`; and, \n",
    "- create a dataframe of the one-hot encoded feature;\n",
    "- delete the original feature; \n",
    "- combine our cleaned data with the one-hot encoded dataframe. \n",
    "\n",
    "Note that missing values will be indicated by all `False` (or 0) values when one-hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Enclosure'] = df_raw['Enclosure'].values\n",
    "df['Enclosure'] = df['Enclosure'].replace('EROPS w AC', 'EROPS AC')\n",
    "df['Enclosure'] = df['Enclosure'].replace('None or Unspecified', np.nan)\n",
    "\n",
    "onehot = pd.get_dummies(df['Enclosure'], prefix='Enclosure', dtype=bool)\n",
    "onehot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Enclosure']\n",
    "df = pd.concat([df, onehot], axis=1)\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model and see if this has had any impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "rf, oob_one_hot = evaluate(X, y, n_estimators=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = importances(rf, X, y)\n",
    "plot_importances(I.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `log(SalePrice)`\n",
    "\n",
    "The last thing we will try is to train our model on the logarithm of the price instead of just the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SalePrice'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(df['SalePrice']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "log_y = np.log(y)\n",
    "\n",
    "rf, oob_log = evaluate(X, log_y, n_estimators=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "So, to recap what we have done, we:\n",
    "- broke up the `saledate` feature into its components: year, month, day, etc.;\n",
    "- added an `age` feature; \n",
    "- ordinal encoded `ProductSize`;\n",
    "- one-hot encoded `Hydraulics_Flow` and `Enclosure`; and, \n",
    "- used the logarithm of `SalePrice` as our new target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After all the techniques we have used in this notebook, we have increased our OOB $R^2$ score from the more stable baseline of 0.903 to 0.919, which is a nice increase. This represents an increase in the explained variance of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{round(100 * ((1 - 0.903) - (1 - 0.919)) / (1 - 0.903))}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Missing_Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
