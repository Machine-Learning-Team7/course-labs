{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION MODEL:\n",
    "Due to nature of this process (creating a predictive model), involving coming back and forth trial and error while cleaning data and testing differet hyperparameters setup. We decided to:\n",
    "\n",
    "- Move all our helper functions to a separated file, so we can reach them from every file while keeping our files cleaner.\n",
    "- Have 2 code files to get our final model:\n",
    "    - File `clf_data_proccessing.ipynb`:<br>\n",
    "        &emsp;To read and clean the data and save it to sql database in ``./database/models.db`` file.<br>\n",
    "        &emsp;To test cleanliness of data, we'll use a random forest model.\n",
    "    - File `clf_model_selection.ipynb` to test and compare different models working with cleaned dataset.\n",
    "- Once final model version is selected, it will be serialized after trainning and stored in ``./trained_models`` folder.\n",
    "- Then trained model will be deployed to a website built with flask/jinja to perform predictions for data entered by users.\n",
    "***\n",
    "\n",
    "### MODEL CREATION\n",
    "With our data cleaned, well try differnent classification models to come up with the model to be deployed in the website.<br>\n",
    "Well test:\n",
    "- Random forest classifier\n",
    "- Knn classifier\n",
    "- Logistic regressor\n",
    "\n",
    "we use cross validation to select the best version for each model, then we just use score method in the model to select the final model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myFunc import *  # importing helper functions\n",
    "# pull cleaned dataset\n",
    "con = sqlite3.connect('./../database/models.db')\n",
    "df=pd.read_sql_query('select * from class_clean_data',con)\n",
    "# separating vector features from target\n",
    "X=df.drop(['num'],axis=1)\n",
    "y=df['num']\n",
    "# pulling out test data, we'll use it after tweeking hyperparameters in different models.\n",
    "X1,Xtest,y1,ytest=train_test_split(X, y, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 81.29120879120877\n",
      "Accuracy -test set: 80.3030303030303\n",
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 81.73076923076923\n",
      "Accuracy -test set: 80.3030303030303\n",
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 83.1868131868132\n",
      "Accuracy -test set: 84.84848484848484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a code from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 ---------\n",
    "# was used to find best hp for our rf model, results are shown in line below, but results weren't too different from those we got with simpler settings.\n",
    "# rf1=RandomForestClassifier(n_estimators=1600,min_samples_split=2,min_samples_leaf=4,max_features='sqrt',max_depth=80,bootstrap=True)\n",
    "\n",
    "rf1=RandomForestClassifier(n_estimators=55, bootstrap=False)\n",
    "cross_val(rf1,X1,y1,'c')\n",
    "rf2=RandomForestClassifier(n_estimators=150)# default\n",
    "cross_val(rf2,X1,y1,'c')\n",
    "rf3=RandomForestClassifier(n_estimators=200)\n",
    "cross_val(rf3,X1,y1,'c')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 61.37362637362638\n",
      "Accuracy -test set: 62.121212121212125\n",
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 62.2252747252747\n",
      "Accuracy -test set: 68.18181818181817\n",
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 63.736263736263744\n",
      "Accuracy -test set: 59.09090909090909\n"
     ]
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=3)\n",
    "cross_val(knn1,X1,y1,'c')\n",
    "knn2 = KNeighborsClassifier(n_neighbors=35)\n",
    "cross_val(knn2,X1,y1,'c')\n",
    "knn3 = KNeighborsClassifier(n_neighbors=17)\n",
    "cross_val(knn3,X1,y1,'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 82.80219780219781\n",
      "Accuracy -test set: 80.3030303030303\n",
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 83.8736263736264\n",
      "Accuracy -test set: 81.81818181818183\n",
      "-------------Cross Validation-----------------\n",
      "Accuracy -val set: 70.27472527472527\n",
      "Accuracy -test set: 66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "lr1=LogisticRegression(solver='lbfgs',penalty='l2',C=.6)\n",
    "cross_val(lr1,X1,y1,'c')\n",
    "lr2=LogisticRegression(solver='newton-cg',penalty='l2',C=.55)\n",
    "cross_val(lr2,X1,y1,'c')\n",
    "lr3=LogisticRegression(solver='sag',penalty=None,C=3)\n",
    "cross_val(lr3,X1,y1,'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9032258064516129\n",
      "0.6774193548387096\n",
      "0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "# print(accuracy_score(ytest,rf3.predict(Xtest)))\n",
    "# print(accuracy_score(ytest,knn2.predict(Xtest)))\n",
    "# print(accuracy_score(ytest,lr2.predict(Xtest)))\n",
    "print(rf3.score(Xtest,ytest))\n",
    "print(knn2.score(Xtest,ytest))\n",
    "print(lr2.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.90        31\n",
      "   macro avg       0.91      0.90      0.90        31\n",
      "weighted avg       0.90      0.90      0.90        31\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88        16\n",
      "           1       0.92      0.80      0.86        15\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.88      0.87      0.87        31\n",
      "weighted avg       0.88      0.87      0.87        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,rf3.predict(Xtest)))\n",
    "print(classification_report(ytest,lr2.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest is the winner, although LogisticRegressor was close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\trained_models\\\\class_heart.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jl_filedir = Path(\"./../trained_models\")\n",
    "jl_filedir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "jl_filepath=jl_filedir / 'class_heart.joblib'\n",
    "\n",
    "joblib.dump(rf3,jl_filepath)\n",
    "\n",
    "# rf3_jl=joblib.load(jl_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
