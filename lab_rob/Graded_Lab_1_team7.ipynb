{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMP2006 -- Graded Lab 1\n",
    "\n",
    "In this lab, you will gain some experience in **denoising** a dataset in the context of a specific objective. \n",
    "\n",
    "**Overall Objective**: Create a model that predicts rent prices as well as possible for typical New York City apartments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data set**: make sure you use the data with the same number as your group number!\n",
    "\n",
    "| Group | Data set |\n",
    "| :-: | :-: |\n",
    "| 1 | rent_1.csv |\n",
    "| 2 | rent_2.csv |\n",
    "| etc. | etc. |\n",
    "\n",
    "**Important Notes:**\n",
    " - This lab is more open-ended so be prepared to think on your own, in a logical way, in order to solve the problem at hand\n",
    "     - You should be able to support any decision you make with logical evidence\n",
    " - The data looks like the data we have been using in class but it has other **surprises**\n",
    "     - Be sure to investigate the data in a way that allows you to discover all these surprises\n",
    " - Use [Chapter 5](https://mlbook.explained.ai/prep.html) of the textbook as a **guide**, except:\n",
    "     - you only need to use **random forest** models;\n",
    "     - exclude Section 5.5; \n",
    " - Code submitted for this lab should be:\n",
    "     - error free\n",
    "         - to make sure this is the case, before submitting, close all Jupyter notebooks, exit Anaconda, reload the lab notebook and execute all cells\n",
    "     - final code\n",
    "         - this means that I don't want to see every piece of code you try as you work through this lab but only the final code; only the code that fulfills the objective\n",
    " - Use the **out-of-bag score** to evaluate models\n",
    "     - Read Section 5.2 carefully so that you use this method properly\n",
    "     - The oob score that you provide should be the average of 10 runs\n",
    " - Don't make assumptions!\n",
    "\n",
    "I have broken the lab down into 4 main parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0\n",
    "\n",
    "Please provide the following information:\n",
    " - Group Number: 7\n",
    " - Group Members\n",
    "     - Manuel Bishop Noriega 4362207\n",
    "     - Robert E. Matney III 4364229\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Create and evaluate an initial model\n",
    "\n",
    "#### Code (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manolo\\AppData\\Local\\Temp\\ipykernel_34452\\60411174.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 16)\n",
      "0.0698\n",
      "OOB score -0.0070\n",
      "Validation MAE trials: $1317 $964 $961 $1022 $941 $1038 $973 \n",
      "Average validation MAE $1031\n"
     ]
    }
   ],
   "source": [
    "# importing panda to the code, to use dataframes.\n",
    "import pandas as pd\n",
    "\n",
    "# getting the csv file rent opened in a variable holding a dataframe\n",
    "df = pd.read_csv(\"rent_7.csv\")\n",
    "\n",
    "# getting how many vectors/records of data are in file\n",
    "print(df.shape)\n",
    "\n",
    "# creating table with just these columns of data with numbers\n",
    "# it would also be possible to convert non-numeric data columns to numbers and added if relevant\n",
    "df_num = df[['bathrooms', 'bedrooms', 'longitude', 'latitude', 'price']]\n",
    "\n",
    "# separating features vector and target columns\n",
    "X_train = df_num.drop('price', axis=1)\n",
    "y_train = df_num['price']\n",
    "\n",
    "# creating an appropriate model with suitable hyper-parameters\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 100, n_jobs = -1)\n",
    "\n",
    "# fit model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# getting a relationship between X_train and y_train\n",
    "r2 = rf.score(X_train, y_train)\n",
    "print(f\"{r2:.4f}\")\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 100, n_jobs = -1, oob_score = True)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "noisy_oob_r2 = rf.oob_score_\n",
    "print(f\"OOB score {noisy_oob_r2:.4f}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "X, y = df_num.drop('price', axis=1), df_num['price']\n",
    "\n",
    "errors = []\n",
    "print(f\"Validation MAE trials: \", end='')\n",
    "for i in range(7):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predicted = rf.predict(X_test)\n",
    "    e = mean_absolute_error(y_test, y_predicted)\n",
    "    print(f\"${e:.0f} \", end='')\n",
    "    errors.append(e)\n",
    "print()\n",
    "noisy_avg_mae = np.mean(errors)\n",
    "print(f\"Average validation MAE ${noisy_avg_mae:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation (5 marks)\n",
    "\n",
    "Please provide an explanation and justification for the code submitted in **Part 1** in the context of the overall objective. \n",
    "\n",
    "To start with the was getting the number of data entery rows and columns in the csv file.\n",
    "\n",
    "Then we are training the machine based on the data that is given to look for a type of outcome. Trying to preform a model to help with trying to find a relationship between all of the data. R2 low score suggested no relationship found or noisiy data. We proceeded to confirm this in the next steps.\n",
    "\n",
    "Next we are trying to use the model and what the machine has learned from the model to predict the average apartmet rent price. \n",
    "\n",
    "Then we did several valiation trails for get different amount of data to get the MAE.\n",
    "\n",
    "From the data that is collected we get a MAE from that.\n",
    "\n",
    "Results from oob score and MAE average validation showed our model performs really bad making predictions. But before concluding there's no relationship between our features vector and target we should make sure our data set is free of inconsistencies, errors or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Denoise the data\n",
    "\n",
    "This section should only include the code necessary to **denoise** the data, NOT the code necessary to identify inconsistencies, problems, errors, etc. in the data. \n",
    "\n",
    "#### Code (25 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOISING DATA, next two lines get rid of outliers or errors regarding apts location and prices.\n",
    "#narrowing prices range to consider only reasonable ones (excludes also negative values)\n",
    "df_clean = df_num[(df_num.price>1_000) & (df_num.price<10_000)]\n",
    "\n",
    "\n",
    "#delimiting coverage area to new york city only.\n",
    "df_clean = df_clean[(df_clean['latitude']>40.55) &\n",
    "        (df_clean['latitude']<40.94) &\n",
    "        (df_clean['longitude']>-74.1) &\n",
    "        (df_clean['longitude']<-73.67)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Create and evaluate a final model\n",
    "\n",
    "#### Code (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation OOB score  0.820219\n",
      "Valiation MAE trials:$356 $356 $359 $349 $350 $358 $351 \n",
      "Average validation MAE $354\n",
      "LM Training score 0.5709\n",
      "LM Validation score 0.5687\n",
      "GB Training score 0.8607\n",
      "GB Validation score 0.7992\n"
     ]
    }
   ],
   "source": [
    "X, y = df_clean.drop('price', axis=1), df_clean['price']\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, #parallelize\n",
    "                    oob_score=True) # get error estimate\n",
    "rf.fit(X, y)\n",
    "clean_oob_r2 = rf.oob_score_\n",
    "print(f\"Validation OOB score {clean_oob_r2: 4f}\")\n",
    "\n",
    "errors = []\n",
    "print(f\"Valiation MAE trials:\", end='')\n",
    "for i in range(7):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predicted = rf.predict(X_test)\n",
    "    e = mean_absolute_error(y_test, y_predicted)\n",
    "    print(f\"${e:.0f} \", end='')\n",
    "    errors.append(e)\n",
    "print()\n",
    "noisy_avg_mae = np.mean(errors)\n",
    "print(f\"Average validation MAE ${noisy_avg_mae:.0f}\")\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "lm = Lasso(alpha=0.5) # create linear model\n",
    "lm.fit(X_train, y_train)\n",
    "print(f\"LM Training score {lm.score(X_train, y_train):.4f}\")\n",
    "print(f\"LM Validation score {lm.score(X_test, y_test):.4f}\")\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators = 2000)\n",
    "gbr.fit(X_train, y_train)\n",
    "print(f\"GB Training score {gbr.score(X_train, y_train):.4f}\")\n",
    "print(f\"GB Validation score {gbr.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation (5 marks)\n",
    "\n",
    "Please provide an explanation and justification for the code submitted in **Part 3** in the context of the overall objective. \n",
    "\n",
    "Now we run our model with denoised dataset (in part 2) to see how much its scores are improved. Results are way better than before, next, we'll compared how this models performs compared to other 2 models\n",
    "\n",
    "Then the Lasso is a linear model to see if it will pull the data in a better way and see it if gives a better score. It does not.\n",
    "\n",
    "Then we tried the gradient boosting model to see if it would give a different score for the data.  It did not do any better either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Document the problems (35 marks)\n",
    "\n",
    "In this part, please use the table below to document your understanding of all the data issues you discovered. Note that **no code** should be included, as that should be covered in **Part 2**. Also, note that even if one line of code fixed a few problems, you should list each problem separately in the table below, so be sure you have investigated the data properly. For example, if the list `[-6, 5, 0, 50]` represents heights of adults, the -6, 0, and 50 would represent three data issues to be included in the table below, even though one line of code may be able to address all of them. \n",
    "\n",
    "| Data issue discovered | Why is this a problem? | How did you fix it? | Why is this fix appropriate? |\n",
    "| :- | :- | :- | :- | \n",
    "| example problem 1: The longitude and Latitude both have some data entry that have 0.  | Which is this impossible because if it was the case then those apartments would not be in the area of New York but somewhere out in the Ocean or the equator. |  So, what needs to be done is to find all the data entry sets that have either the longitude or latitude set as 0 (or other outliers) and remove them from the model. | This fix is appropriate because those data sets with those number could be good data in the rest of the columns. The data could have been entered wrong or mistyped. |\n",
    "|  example problem 2: The such degree in the price being so low to the price being so high to get the mead of the price of apartments rentals. | The problem with there be extreme low prices and extreme high prices is that it throws off the mean average price for the apartments rentals.    | So in order to fix this we filtered the data to get only the records with appartment prices between $1,000 and $10,000. | What this will do is help remove any mistakes that could have been made in either mistyping or just given the wrong data by accident. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
